{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFtftR96UCLY",
        "outputId": "707bc3f7-f773-4ffa-991e-0f16ef5369c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement math (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for math\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pandas math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    images, labels, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMl3DNK499aP",
        "outputId": "85aba886-e945-45a4-fe11-bab5e32deaf3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (2267, 50, 37)\n",
            "Testing data shape: (756, 50, 37)\n",
            "Training labels shape: (2267,)\n",
            "Testing labels shape: (756,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import numpy as np\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSfgLPUsUV_o",
        "outputId": "650e3ec1-cae5-41a6-a8d5-b58362e2e690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Convert image patches to embeddings\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, H, W)\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n",
        "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JQHgobBnUcRM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "    def __init__(self, embed_dim=768, n_heads=12, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention calculation\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "I8pdIUQbUik5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi-layer perceptron block\"\"\"\n",
        "    def __init__(self, embed_dim=768, hidden_dim=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.gelu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GzvF25WPUmJA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block with attention and MLP\"\"\"\n",
        "    def __init__(self, embed_dim=768, n_heads=12, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, n_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm design\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "eVr7n3kGUqBP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer for morphed image detection\"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3,\n",
        "                 embed_dim=768, depth=12, n_heads=12, mlp_ratio=4,\n",
        "                 num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.n_patches = self.patch_embed.n_patches\n",
        "\n",
        "        # Class token and positional embeddings\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, n_heads, mlp_ratio, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
        "\n",
        "        # Add class token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        cls_token_final = x[:, 0]  # Extract class token\n",
        "\n",
        "        if return_features:\n",
        "            return cls_token_final\n",
        "\n",
        "        # Classification\n",
        "        logits = self.head(cls_token_final)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "cl5S-mEAUsoY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator for Multi-Collaborative GAN\"\"\"\n",
        "    def __init__(self, latent_dim=100, img_channels=3, img_size=224):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.init_size = img_size // 4  # Initial size before upsampling\n",
        "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, img_channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.l1(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img"
      ],
      "metadata": {
        "id": "qLpvbbdAUvRM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Single discriminator for collaborative training\"\"\"\n",
        "    def __init__(self, img_channels=3, img_size=224):\n",
        "        super().__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, 2, 1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(img_channels, 16, normalize=False),\n",
        "            *discriminator_block(16, 32),\n",
        "            *discriminator_block(32, 64),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128, 256),\n",
        "        )\n",
        "\n",
        "        # Calculate the size of the flattened features\n",
        "        ds_size = img_size // 2 ** 5  # 5 downsampling layers\n",
        "        self.adv_layer = nn.Sequential(\n",
        "            nn.Linear(256 * ds_size ** 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Feature extraction layer for ensemble\n",
        "        self.feature_layer = nn.Linear(256 * ds_size ** 2, 512)\n",
        "\n",
        "    def forward(self, img, return_features=False):\n",
        "        features = self.model(img)\n",
        "        features_flat = features.view(features.shape[0], -1)\n",
        "\n",
        "        if return_features:\n",
        "            return self.feature_layer(features_flat)\n",
        "\n",
        "        validity = self.adv_layer(features_flat)\n",
        "        return validity"
      ],
      "metadata": {
        "id": "TmyRQXdyU1Xt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiCollaborativeGAN(nn.Module):\n",
        "    \"\"\"Multi-Collaborative GAN with multiple discriminators\"\"\"\n",
        "    def __init__(self, num_discriminators=3, latent_dim=100, img_channels=3, img_size=224):\n",
        "        super().__init__()\n",
        "        self.num_discriminators = num_discriminators\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Generator\n",
        "        self.generator = Generator(latent_dim, img_channels, img_size)\n",
        "\n",
        "        # Multiple discriminators\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            Discriminator(img_channels, img_size) for _ in range(num_discriminators)\n",
        "        ])\n",
        "\n",
        "        # Feature fusion layer\n",
        "        self.feature_fusion = nn.Sequential(\n",
        "            nn.Linear(512 * num_discriminators, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Extract features from all discriminators\n",
        "        disc_features = []\n",
        "        disc_scores = []\n",
        "\n",
        "        for discriminator in self.discriminators:\n",
        "            features = discriminator(x, return_features=True)\n",
        "            scores = discriminator(x, return_features=False)\n",
        "            disc_features.append(features)\n",
        "            disc_scores.append(scores)\n",
        "\n",
        "        # Concatenate features from all discriminators\n",
        "        combined_features = torch.cat(disc_features, dim=1)\n",
        "        fused_features = self.feature_fusion(combined_features)\n",
        "\n",
        "        if return_features:\n",
        "            return fused_features\n",
        "\n",
        "        # Average discriminator scores\n",
        "        avg_score = torch.mean(torch.stack(disc_scores), dim=0)\n",
        "        return avg_score\n",
        "\n",
        "    def generate_samples(self, batch_size):\n",
        "        \"\"\"Generate samples using the generator\"\"\"\n",
        "        z = torch.randn(batch_size, self.latent_dim, device=next(self.parameters()).device)\n",
        "        return self.generator(z)"
      ],
      "metadata": {
        "id": "axenlg0xU4Ik"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleMetaModel(nn.Module):\n",
        "    \"\"\"Meta-model for ensemble learning combining ViT and Multi-Collaborative GAN\"\"\"\n",
        "    def __init__(self, vit_feature_dim=768, gan_feature_dim=128, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature normalization layers\n",
        "        self.vit_norm = nn.LayerNorm(vit_feature_dim)\n",
        "        self.gan_norm = nn.LayerNorm(gan_feature_dim)\n",
        "\n",
        "        # Feature fusion network\n",
        "        total_features = vit_feature_dim + gan_feature_dim\n",
        "        self.fusion_network = nn.Sequential(\n",
        "            nn.Linear(total_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Final classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "        # Attention-based weighting\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(total_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2),  # 2 weights for ViT and GAN\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, vit_features, gan_features):\n",
        "        # Normalize features\n",
        "        vit_features = self.vit_norm(vit_features)\n",
        "        gan_features = self.gan_norm(gan_features)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined_features = torch.cat([vit_features, gan_features], dim=1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_weights = self.attention(combined_features)\n",
        "\n",
        "        # Apply attention weighting\n",
        "        weighted_vit = vit_features * attention_weights[:, 0:1]\n",
        "        weighted_gan = gan_features * attention_weights[:, 1:2]\n",
        "\n",
        "        # Combine weighted features\n",
        "        final_features = torch.cat([weighted_vit, weighted_gan], dim=1)\n",
        "\n",
        "        # Feature fusion\n",
        "        fused_features = self.fusion_network(final_features)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(fused_features)\n",
        "\n",
        "        return logits, attention_weights"
      ],
      "metadata": {
        "id": "dZK-KUQwU6HN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MorphDetectionEnsemble(nn.Module):\n",
        "    \"\"\"Complete ensemble model for morphed image detection\"\"\"\n",
        "    def __init__(self, img_size=224, num_classes=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # ViT models (B/16 and L/32 variants)\n",
        "        self.vit_b16 = VisionTransformer(\n",
        "            img_size=img_size, patch_size=16, embed_dim=768,\n",
        "            depth=12, n_heads=12, num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        self.vit_l32 = VisionTransformer(\n",
        "            img_size=img_size, patch_size=32, embed_dim=1024,\n",
        "            depth=24, n_heads=16, num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # Multi-Collaborative GAN\n",
        "        self.mc_gan = MultiCollaborativeGAN(\n",
        "            num_discriminators=3, img_size=img_size\n",
        "        )\n",
        "\n",
        "        # Meta-models for different combinations\n",
        "        self.meta_model_b16_gan = EnsembleMetaModel(\n",
        "            vit_feature_dim=768, gan_feature_dim=128, num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        self.meta_model_l32_gan = EnsembleMetaModel(\n",
        "            vit_feature_dim=1024, gan_feature_dim=128, num_classes=num_classes\n",
        "        )\n",
        "\n",
        "        # Final ensemble fusion\n",
        "        self.final_fusion = nn.Sequential(\n",
        "            nn.Linear(num_classes * 2, 128),  # 2 meta-models\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from ViT models\n",
        "        vit_b16_features = self.vit_b16(x, return_features=True)\n",
        "        vit_l32_features = self.vit_l32(x, return_features=True)\n",
        "\n",
        "        # Extract features from Multi-Collaborative GAN\n",
        "        gan_features = self.mc_gan(x, return_features=True)\n",
        "\n",
        "        # Meta-model predictions\n",
        "        meta1_logits, attention1 = self.meta_model_b16_gan(vit_b16_features, gan_features)\n",
        "        meta2_logits, attention2 = self.meta_model_l32_gan(vit_l32_features, gan_features)\n",
        "\n",
        "        # Combine meta-model predictions\n",
        "        combined_logits = torch.cat([meta1_logits, meta2_logits], dim=1)\n",
        "\n",
        "        # Final ensemble prediction\n",
        "        final_logits = self.final_fusion(combined_logits)\n",
        "\n",
        "        return {\n",
        "            'final_logits': final_logits,\n",
        "            'meta1_logits': meta1_logits,\n",
        "            'meta2_logits': meta2_logits,\n",
        "            'attention1': attention1,\n",
        "            'attention2': attention2,\n",
        "            'vit_b16_features': vit_b16_features,\n",
        "            'vit_l32_features': vit_l32_features,\n",
        "            'gan_features': gan_features\n",
        "        }"
      ],
      "metadata": {
        "id": "mG3Gwp9OU8Oe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleLoss(nn.Module):\n",
        "    \"\"\"Combined loss function for ensemble training\"\"\"\n",
        "    def __init__(self, alpha=0.6, beta=0.2, gamma=0.2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # Weight for final loss\n",
        "        self.beta = beta    # Weight for meta-model 1 loss\n",
        "        self.gamma = gamma  # Weight for meta-model 2 loss\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.consistency_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Main classification losses\n",
        "        final_loss = self.criterion(outputs['final_logits'], targets)\n",
        "        meta1_loss = self.criterion(outputs['meta1_logits'], targets)\n",
        "        meta2_loss = self.criterion(outputs['meta2_logits'], targets)\n",
        "\n",
        "        # Consistency loss between meta-models\n",
        "        consistency = self.consistency_loss(\n",
        "            F.softmax(outputs['meta1_logits'], dim=1),\n",
        "            F.softmax(outputs['meta2_logits'], dim=1)\n",
        "        )\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = (self.alpha * final_loss +\n",
        "                     self.beta * meta1_loss +\n",
        "                     self.gamma * meta2_loss +\n",
        "                     0.1 * consistency)\n",
        "\n",
        "        return total_loss, {\n",
        "            'final_loss': final_loss.item(),\n",
        "            'meta1_loss': meta1_loss.item(),\n",
        "            'meta2_loss': meta2_loss.item(),\n",
        "            'consistency_loss': consistency.item()\n",
        "        }"
      ],
      "metadata": {
        "id": "en2iP55HU-ic"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"Train the ensemble model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss, loss_dict = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs['final_logits'].max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}: Loss = {loss.item():.4f}, Acc = {100.*correct/total:.2f}%')\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "BpgyP0g6VB1P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ensemble(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluate the ensemble model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss, _ = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs['final_logits'].max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "8CmVMz0ZVD7l"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Example dataset loading (modify paths as needed)\n",
        "train_dataset = X_train\n",
        "val_dataset = X_test\n",
        "\n",
        "train_loader = y_train\n",
        "val_loader = y_test\n",
        "\n",
        "print(\"Data loading setup complete. Update dataset paths as needed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJfTlJ7SVFy2",
        "outputId": "ef08fe82-a163-4036-b842-9f707257c8e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading setup complete. Update dataset paths as needed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the complete ensemble model\n",
        "model = MorphDetectionEnsemble(img_size=224, num_classes=2).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = EnsembleLoss(alpha=0.6, beta=0.2, gamma=0.2)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "# Model summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model initialized successfully!\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSmVd5noVHnr",
        "outputId": "22ddc276-9f0a-4cd8-cef5-8860f812125a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized successfully!\n",
            "Total parameters: 455,739,748\n",
            "Trainable parameters: 455,739,748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop (example - modify as needed)\n",
        "def train_ensemble_model(model, train_loader, val_loader, num_epochs=50):\n",
        "    \"\"\"Complete training loop for the ensemble model\"\"\"\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training\n",
        "        train_loss, train_acc = train_ensemble_epoch(\n",
        "            model, train_loader, optimizer, criterion, device\n",
        "        )\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_ensemble(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "            }, 'best_ensemble_model.pth')\n",
        "            print(f'New best model saved with validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "    print(f'\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%')\n",
        "    return model\n",
        "\n",
        "# Uncomment to start training (ensure you have data loaders)\n",
        "#trained_model = train_ensemble_model(model, train_loader, val_loader, num_epochs=50)\n",
        "\n",
        "print(\"Training function ready. Uncomment the line above to start training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YEm8UKiVNW6",
        "outputId": "74d3e8ce-a6f0-41d3-880f-d8ae4821b944"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training function ready. Uncomment the line above to start training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_image(model, image_path, transform, device):\n",
        "    \"\"\"Predict on a single image\"\"\"\n",
        "    from PIL import Image\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "\n",
        "        # Get probabilities\n",
        "        probabilities = F.softmax(outputs['final_logits'], dim=1)\n",
        "        prediction = probabilities.argmax(dim=1).item()\n",
        "        confidence = probabilities.max().item()\n",
        "\n",
        "        # Get attention weights\n",
        "        attention1 = outputs['attention1'].cpu().numpy()[0]\n",
        "        attention2 = outputs['attention2'].cpu().numpy()[0]\n",
        "\n",
        "    result = {\n",
        "        'prediction': 'Morphed' if prediction == 1 else 'Genuine',\n",
        "        'confidence': confidence,\n",
        "        'probabilities': probabilities.cpu().numpy()[0],\n",
        "        'attention_weights_1': attention1,\n",
        "        'attention_weights_2': attention2\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "#result = predict_single_image(model, 'path/to/image.jpg', val_transforms, device)\n",
        "# print(f\"Prediction: {result['prediction']} (Confidence: {result['confidence']:.3f})\")\n",
        "\n",
        "print(\"Inference function ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufdu7jHPVRXx",
        "outputId": "25c7df89-973d-410a-928f-2268d933f6c6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference function ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture summary\n",
        "def print_model_summary(model):\n",
        "    \"\"\"Print detailed model architecture summary\"\"\"\n",
        "    print(\"Ensemble Model Architecture Summary:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"1. Vision Transformer B/16:\")\n",
        "    print(f\"   - Patch size: 16x16\")\n",
        "    print(f\"   - Embedding dimension: 768\")\n",
        "    print(f\"   - Transformer blocks: 12\")\n",
        "    print(f\"   - Attention heads: 12\")\n",
        "\n",
        "    print(\"2. Vision Transformer L/32:\")\n",
        "    print(f\"   - Patch size: 32x32\")\n",
        "    print(f\"   - Embedding dimension: 1024\")\n",
        "    print(f\"   - Transformer blocks: 24\")\n",
        "    print(f\"   - Attention heads: 16\")\n",
        "\n",
        "    print(\"3. Multi-Collaborative GAN:\")\n",
        "    print(f\"   - Number of discriminators: 3\")\n",
        "    print(f\"   - Feature dimension: 128\")\n",
        "    print(f\"   - Collaborative training: Yes\")\n",
        "\n",
        "    print(\"4. Meta-Models:\")\n",
        "    print(f\"   - Meta-model 1: ViT-B/16 + GAN\")\n",
        "    print(f\"   - Meta-model 2: ViT-L/32 + GAN\")\n",
        "    print(f\"   - Attention mechanism: Yes\")\n",
        "\n",
        "    print(\"5. Final Ensemble:\")\n",
        "    print(f\"   - Fusion strategy: Feature-based super learning\")\n",
        "    print(f\"   - Output classes: 2 (Genuine/Morphed)\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "\n",
        "print_model_summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTcEXVmyVcwL",
        "outputId": "8b8f8f7e-37fd-425c-be08-fab32535049f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Architecture Summary:\n",
            "==================================================\n",
            "1. Vision Transformer B/16:\n",
            "   - Patch size: 16x16\n",
            "   - Embedding dimension: 768\n",
            "   - Transformer blocks: 12\n",
            "   - Attention heads: 12\n",
            "2. Vision Transformer L/32:\n",
            "   - Patch size: 32x32\n",
            "   - Embedding dimension: 1024\n",
            "   - Transformer blocks: 24\n",
            "   - Attention heads: 16\n",
            "3. Multi-Collaborative GAN:\n",
            "   - Number of discriminators: 3\n",
            "   - Feature dimension: 128\n",
            "   - Collaborative training: Yes\n",
            "4. Meta-Models:\n",
            "   - Meta-model 1: ViT-B/16 + GAN\n",
            "   - Meta-model 2: ViT-L/32 + GAN\n",
            "   - Attention mechanism: Yes\n",
            "5. Final Ensemble:\n",
            "   - Fusion strategy: Feature-based super learning\n",
            "   - Output classes: 2 (Genuine/Morphed)\n",
            "\n",
            "Total Parameters: 455,739,748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_complete_model(model, optimizer, epoch, accuracy, filepath='morphed_detection_ensemble.pth'):\n",
        "    \"\"\"Save the complete model with all components\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'accuracy': accuracy,\n",
        "        'model_config': {\n",
        "            'img_size': 224,\n",
        "            'num_classes': 2,\n",
        "        }\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {filepath}\")\n",
        "\n",
        "def load_complete_model(filepath='morphed_detection_ensemble.pth', device='cpu'):\n",
        "    \"\"\"Load the complete model\"\"\"\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MorphDetectionEnsemble(\n",
        "        img_size=checkpoint['model_config']['img_size'],\n",
        "        num_classes=checkpoint['model_config']['num_classes']\n",
        "    ).to(device)\n",
        "\n",
        "    # Load state dict\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    print(f\"Model loaded from {filepath}\")\n",
        "    print(f\"Training epoch: {checkpoint['epoch']}\")\n",
        "    print(f\"Best accuracy: {checkpoint['accuracy']:.2f}%\")\n",
        "\n",
        "    return model, checkpoint\n",
        "\n",
        "# Example usage:\n",
        "# save_complete_model(model, optimizer, epoch=0, accuracy=0.0)\n",
        "# loaded_model, checkpoint = load_complete_model()\n",
        "\n",
        "print(\"Model save/load functions ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foIJQLQiVeyv",
        "outputId": "bce5f347-6726-4e75-ba32-1b22ec0f88bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model save/load functions ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model with random input (for verification)\n",
        "def test_model_forward_pass():\n",
        "    \"\"\"Test the model with random input to verify architecture\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Create random input\n",
        "    test_input = torch.randn(2, 3, 224, 224).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(test_input)\n",
        "\n",
        "    print(\"Model Forward Pass Test:\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Input shape: {test_input.shape}\")\n",
        "    print(f\"Final logits shape: {outputs['final_logits'].shape}\")\n",
        "    print(f\"Meta1 logits shape: {outputs['meta1_logits'].shape}\")\n",
        "    print(f\"Meta2 logits shape: {outputs['meta2_logits'].shape}\")\n",
        "    print(f\"Attention1 shape: {outputs['attention1'].shape}\")\n",
        "    print(f\"Attention2 shape: {outputs['attention2'].shape}\")\n",
        "    print(f\"ViT-B/16 features shape: {outputs['vit_b16_features'].shape}\")\n",
        "    print(f\"ViT-L/32 features shape: {outputs['vit_l32_features'].shape}\")\n",
        "    print(f\"GAN features shape: {outputs['gan_features'].shape}\")\n",
        "\n",
        "    # Test probabilities\n",
        "    probs = F.softmax(outputs['final_logits'], dim=1)\n",
        "    print(f\"\\nSample predictions (probabilities):\")\n",
        "    for i in range(test_input.size(0)):\n",
        "        pred_class = probs[i].argmax().item()\n",
        "        confidence = probs[i].max().item()\n",
        "        class_name = 'Morphed' if pred_class == 1 else 'Genuine'\n",
        "        print(f\"  Sample {i+1}: {class_name} (confidence: {confidence:.3f})\")\n",
        "\n",
        "    print(\"\\nModel architecture test completed successfully!\")\n",
        "\n",
        "# Run the test\n",
        "test_model_forward_pass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmH6ldneVhIQ",
        "outputId": "9d4b30a0-fb73-420e-fa55-677aa0b99127"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Forward Pass Test:\n",
            "==============================\n",
            "Input shape: torch.Size([2, 3, 224, 224])\n",
            "Final logits shape: torch.Size([2, 2])\n",
            "Meta1 logits shape: torch.Size([2, 2])\n",
            "Meta2 logits shape: torch.Size([2, 2])\n",
            "Attention1 shape: torch.Size([2, 2])\n",
            "Attention2 shape: torch.Size([2, 2])\n",
            "ViT-B/16 features shape: torch.Size([2, 768])\n",
            "ViT-L/32 features shape: torch.Size([2, 1024])\n",
            "GAN features shape: torch.Size([2, 128])\n",
            "\n",
            "Sample predictions (probabilities):\n",
            "  Sample 1: Morphed (confidence: 0.504)\n",
            "  Sample 2: Morphed (confidence: 0.504)\n",
            "\n",
            "Model architecture test completed successfully!\n"
          ]
        }
      ]
    }
  ]
}